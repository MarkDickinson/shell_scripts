<HTML>
<HEAD><TITLE>Security Configuration Management Toolkit</TITLE></HEAD>
<BODY>
<center>
<b>This documentation is for</b><br />
Version 0.25 of the processing script<br />
Version 0.25 of the data collection script<br />
Last updated 10 February 2026
</center>
<a name="sect0"></a>
<H1>Index</H1>
<p>
<ol>
		<li><a href="#sect1">Security Configuration Toolkit Overview</a></li>
                <ul>
                    <li>Requirements, Packages needed</li>
                    <li>Requirements, environment needed</li>
                    <li>Toolkit Overview</li>
                </ul>
		<li><a href="#sect2">Performing a server data capture</a>
		<ul>
				<li>Capturing data for a server</li>
				<li>Limiting the data captured</li>
				<li>All collection parameters explained</li>
		</ul>
		</li>
		<li><a href="#sect3">Processing the data</a>
		<ul>
			<li>Running the reporting script</li>
			<ul>
				<li>Collecting the data to be processed</li>
				<li>Processing the data</li>
				<li>Automating Report Archiving</li>
<li>Processing only a single server</li>
<li>Processing only updated servers</li>
			</ul>
			<li>Overiding/Customising the default reporting</li>
		</ul>
		</li>
		<li><a href="#sect4">Viewing the security report output</a></li>
		<li><a href="#sect5">Appendix A - Reporting Override Custom File Syntax</a></li>
		<ul>
				<li>USING CUSTOM FILES
				<ul>
			  	  <li>1. Why Custom Files</li>
				  <li>2. Network Port Overrides</li>
				  <li>3. Network Overrides by process</li>
				  <li>4. Home Directory Ownership Overrides</li>
				  <li>5. Home Directory Permission Overrides</li>
				  <li>5.1 General System Directory Overrides</li>
				  <li>5.2 Explicit Directory Overrides</li>
				  <li>6. System File Checks</li>
				  <li>6.1 Forcing a system file into OK state</li>
				  <li>6.2 Managing "sloppy" /var checking</li>
                                  <li>6.3 Permitting files under /var to be group writeable</li>
				  <li>7. Turning off warnings for manual check items</li>
				  <li>8. Turning off alerts for known/ok SUID files</li>
		        </ul>
				</li>
		</ul>
		<li><a href="#sect6">Appendix B - Customisation File settings available</a></li>
		<li><a href="#sect7">Appendix C - Quick Start Guide</a> (if you don't read manuals)</li>
		<li><a href="#sect8">Appendix D - Known Complications with using this toolkit</a></li>
</ol>
</p>

<hr>
<a name="sect1">
<center><a href="#sect0">[Index]</a> <a href="#sect2">[Next Section]</a></center>
<H1>1. Security Configuration Toolkit Overview</H1>

<h3>1.1 Requirements, packages needed</h3>
<ul>
<li>(required) the BASH shell <b>/bin/bash</b> must be available on all servers, the scripts make use of the
bash substring functions not available in other shells</li>
<li>(required) the <b>netstat</b> command must be installed on all servers for collecting the tcp/udp ports
in use</li>
<li>(optional) <b>dmidecode</b> and <b>lshw</b> commands should be installed on all servers for collecting
the hardware details. If they are not installed the only information on the hardware details
page will be that they are not installed</li>
</ul>

<h3>1.2 Requirements, environment needed</h3>
<ul>
<li>a large bunch of Linux servers. These scripts are tested on Fedora33, Rocky8, Alma9,
CentOS8, Debian 12 and Debian13. Also partial support for OpenIndiana has been implemented</li>
<li>server names must be unique until the first '.' (dot) in a server name</li>
<li>server names cannot contain the '_' underscore character (used for parsing)</li>
<li>the scripts cannot be installed in a directory where any part of the directoty path contains the '_' underscore character (used for parsing)</li>
<li>patience, processing a 'full' scanlevel result for a server can easily require checking well over 200,000 files and take over an hour per server</li>
</ul>

<h3>1.3 Toolkit Overview</h3>
<p>
This toolkit does basic security checks of a <b><big>Linux</big></b> server.
It is comprised of two parts, a data collection script to run on each server,
and a processing script to process the collected files on a central reporting
server.
The data collection script must be run as root on the servers to ensure it has
access to all the files it is checking. The processing script should not be run as root.
</p>
<p>
It was created for the Linux servers I use (rhel family and Debian), although partial
SunOS support now exists for the OpenIndiana server I run. <em>Do not expect it to
work correctly on other Linux distributions</em> (they all put files in different places,
even Ubuntu is too different from Debian these days).
</p>
<p>
It is designed for processing results from multiple servers,
written as I have multiple servers and they were all starting 
to digress from ideal situations. Using this I can keep them
all in a reasonably similar secure state (which in many cases 
requires chmod'ing packages installed from repositories as some
of them have terribly insecure file permissions).<br />
I do not believe products such as puppet/chef/cfengine are suitable
for keeping file systems secure as while they can manage products/classes
they know about they cannot manage all the wild things users can
get up to in their unmanaged development filesystems. This toolkit is designed 
to capture everything, especially the applications and badly secured
files users may put in system directories outside managed infrastructure.
</p>
<p>
The reports produced are in HTML format. A master index
summarises violations across all servers, and provides 
links down to the details for each server.
</p>
<p>
It checks for
</p>
<ul>
<li>user sanitation checks, .ssh rc or config files present , are they in ftpusers (or users in ftpusers that are no longer valid users) etc</li>
<li>some system settings, password minlen+expiry, motd</li>
<li>server environment such as firewall running or not, SELinux installed or not</li>
<li>if a firewall any unexpected firewall ports open</li>
<li>badly secured system files (many packages have bad install settings), orphaned files etc.</li>
<li>sshd config, and any config or user scripts they may run on a ssh connection etc)</li>
<li>cron script file securities (excluding anacron jobs), make sure scripts run by cron are only editable by the crontab user etc</li>
<li>suid file checks to find unexpected ones (with customisation flags to suppress alerts if desired
 for all the ones it will always find in docker image filesystems and snap packages; which are still
 a risk of course)</li>
<li>and it checks customistion files used; entries for users, suid files, firewall ports etc. that
 are in the customisation file but do not actually exist on the server are also alerted on so
 you can fic customisation files</li>
</ul>
<p>
The easiest way to see what it does is to run it.
</p>
<p>
It allows customisation of configurable rules on a
global, or per server basis. The customiation configuration files support
'include' of configuration snippets, so if you had for example three servers
running mariadb their customisation files would have a one line include of
a mariadb config rather than having to code the full rules in each servers config
file.
</p>

<p>
<pre>
Basic file structure of the toolkit is
  ../bin
  ../custom
  ../custom_includes
  ../doc
  ../results 
  ../archive [optional, if use use the --archivedir flag with that dir]
</pre>
<ul>
<li>The bin directory contains the scripts.</li>
<li>The custom directory is where you customise processing rules for a server.</li>
<li>The custom_includes directory has common sets of files that can be "included" in
	a server custom file (ie: include server_Debian12 or server_RHEL8|9 and applications
	like mariadb, nrpe, bacula-fd etc) so common files and network ports for each only have to be created once and
	a include line used in each server custom file rather than having to be replicated in each server custom file</li>
<li>The doc directory has this documentation.</li>
<li>The results directory is only created after you have performed a processing run,
it is important to note that on a "full" processing run the results directory
is emptied and recreated from scratch, on a single server run only files in the
affected servers results directory are removed and recreated.</li>
</ul>
</p>
<p>
<em>Additional note</em>: the collection script has an option to tar
up /etc and take rpm (rhel) or apt (debian) package
snapshots. Thats because this toolkit
is becoming part of a global configuration toolkit in which
security is but a small part.<br />
The collection script also captures hardware details <em>if the
required packages to do so are installed on each server</em> which are
available to view for each server from a link on each servers 
report overview page.
</p>

<p>
<b>A critical note for the security concious</b> is that the data collection script 
captures your /etc/passwd and /etc/shadow contents; so make sure the processing server
you copy the data to in order for it to be processed is in a secure zone.
</p>

<p>
In default operation the processing script will rebuild the entire results
directories and contents from scratch by processing every server there are
collected data files for.<br />
How I use it, and the recomended way for regular use, is to have a job that
runs daily with the flag to only process changed data capture files, and stagger
the data collection across servers so only 3-4 need to be processed each day
rather than dozens, especially relevant if only a few files have changed as
reporocessing unchanged files is a bit pointless.<br />
It is also possible to run processing against a single
server by name and merge the new results into the full report structure but there
are considerations to be aware of when doing so which are discussed in a later
section, additional servers may be processed if needed.
</p>

<hr>
<a name="sect2"></a>
<center><a href="#sect1">[Previous Section]</a> <a href="#sect0">[Index]</a> <a href="#sect3">[Next Section]</a></center>
<H1>2. Performing a server data capture</H1>
<p>
Obviously there needs to be data to process to produce
a report. So how to we obtain it ?.
</p>
<h2>Capturing data for a server</h2>
<p>
Obtaining the data to be processed is simple to achieve,
simply...
<ol>
<li>copy the bin/collect_server_details.sh script to each server</li>
<li>as the root user (to gain access to files being recorded), run it</li>
<li>copy the *.txt (and optional *.tar) files produced in your current directory to your
reporting servers incoming data directory.</li>
</ol>
You should try to automate the running of the data capture so
it runs at regular intervals. Then on the reporting server you
can setup a job to pull the data from each remote server on
regular intervals and produce a refreshed report.
</p>

<h2>Limiting the data captured</h2>
<p>
Why would you want to ?. Well it collects a lot of data which
results in a long processing time. I do a full scan infrequently
 with a smaller scan regularly.
</p>
<p>
By default the script will record all file permissions under
system (and selected user) directories. This can result in well
over 50,000 file permissions to be checked (I believe the top I hit was around 170,000 files),
and result in an extremely long processing time per server.
</p>
<p>
This should be allowed at least the first time you run it. I was
supprised at how many bad (orphaned) files it found on my server
after I cleaned up the passwd file.
</p>
<p>
After the first run, you may for future runs just want to chain
down N levels of directories in the permission check searches.
To do this provide the parameter '--scanlevel=<n>' to select the
number of directory levels that will be descended during the file
permission search.
<b>It is important to note that overriding the scanlevel only affects file
permision checks, checks for suid files will always traverse full
file paths as these must be reported on</b>.
</p>
<pre>
Normal syntax          : <span style="background-color: #FFCCFF">bin/collect_server_details.sh</span>
Limiting capture syntax: <span style="background-color: #FFCCFF">bin/collect_server_details.sh --scanlevel=<em><b>n</b></em></span>
Example                : <span style="background-color: #D7FFFF">bin/collect_server_details.sh --scanlevel=5</span>
</pre>

<h2>All collection parameters explained</h2>
<p>
These are all the parameters available to the collection script.
</p>
<pre>
bin/collect_server_details.sh [--scanlevel=<number>] [--backup-etc=yes|no] [--record-packages=yes|no] [--hwlist=yes|no] [--webpathlist=filename]
</pre>
<ul>
<li>--scanlevel=N<br />
Limit the number of directory paths depth down which file security information is collected, default is to scan to the max permitted by the OS
</li>
<li>--backup-etc=yes|no<br />
Controls if the script also backs up the contents of /etc, default is no
</li>
<li>--record-packages=yes|no<br />
Controls if the script also creates a listing (if rpm is installed) of all packages installed on the server, default is yes
</li>
<li>--hwlist=yes|no<br />
Controls if a hardware listing of the server is also captured, default is yes
</li>
<li>--webpathlist=filename<br />
If used references a file containing a list of directories that are to be considered webserver served directories
that are required to have all files secured as read only. The file contains a list of full path directory names
(with no trailing /), one directory name per line. The # character in column 1 can be used to place comments
in the file. If this option is used when the processing script processes data collected with this option
a seperate 'appendix W' in the security report is produced to alert on files under these directory paths
that are not read only. The default is that it is not used.<br />
<em>This could be used for any directory path that should contain read-only files such as package repositories</em>
</li>
</ul>

<hr>
<a name="sect3"></a>
<center><a href="#sect2">[Previous Section]</a> <a href="#sect0">[Index]</a> <a href="#sect4">[Next Section]</a></center>
<H1>3. Processing the data</H1>
<H2>Running the reporting script</H2>
<p>
The four required steps to produce a security report are
<ol>
	<li>Collecting the data to be processed, discussed above</li>
	<li>Processing the data</li>
	<li>Automating report archiving</li>
	<li>Viewing the reports produced</li>
</ol>
<p>
Often you will not want to re-process all servers just to test changes made to one, 
and for performance reasons you are likely to want to process only a few servers a
say; so the following processing options are also available.
</p>
<ul>
	<li>Processing only a single server</li>
	<li>Processing only updated servers</li>
</ul>
</p>
<h3>Collecting the data to be processed</h3>
<p>
This was covered in the earlier section. It is mentioned again
here to ensure you understand that <b>all servers should have
had the capture script run on them, and the data transferred
to the reporting server <em>before</em> processing of any
data is performed</b>.
</p>
<p>
You should have run the data capture script discussed earlier
on each server you intend to produce a report for. The results
of the data collection <b>from all servers</b> need to be collected
together in one directory on your reporting server.
</p>
<p>
This is because the script is designed to produce a consolidated report
of all the servers from a single processing run, providing a global
summary page for allservers and drill down to each server for details
of the alerts or warnings produced; so all data collected must be processed on 
a single reporting server.
</p>

<h3>Processing the data</h3>
<p>
To process the data and produce the security reports you just
run the script bin/process_server_details and provide it with
the directory name of your server data files. The syntax of the
process_server_details.sh script is one of the below

<pre>
<span style="background-color: #FFCCFF">
process_server_details.sh --datadir=directory [--archivedir=directory] [--oneserver=servername]
process_server_details.sh --datadir=directory [--archivedir=directory] [--checkchanged=list|process]
process_server_details.sh --datadir=directory --clearlock
process_server_details.sh --datadir=directory --indexonly [--indexkernel=yes|no]
</span>
</pre>
<ul>
<li>The <em>--datadir=dirname</em> specified the location of the files to be processed, these
are the files produced by the collection script</li>
<li>If the <em>--archivedir=directory</em> is provided then when the report is produced
a copy of it will be tar'ed and gzip'ed into a datestamped file
in the archive directory for historical reference.</li>
<li>The <em>--oneserver=servername</em> option has will process only the specified individual one server
which is useful when you are concentrating on resolving issues on one server at a time,
<em>but has special considerations discussed further below</em></li>
<li>The <em>--checkchanged</em> option is used to list or process servers that have collected datafiles
produced after the last processing for the server was run. This fills the gap between processing all
servers and an individual server</li>
<li>The <em>--indexkernel=yes|no</em> can be used on any processing option including the --indexonly option,
if used the main index page will also display the kernel version running on each server processed. The
default is 'no'</li>
</ul>
</p>
<p>
Example: .......<br>
  assuming your data is in the rawdatafiles folder under the
  application directory, and you are in the application
  directory...<br>
<span style="background-color: #D7FFFF"><b>bin/process_server_details.sh --datadir=rawdatafiles</b><br></span>
  When completed the results/index.html file will have an
  overall scoreboard for each server and links to each servers
  detailed reports.
</p>
<p>
<b>I M P O R T A N T :</b><br>
  Don't try to process files in a directory where the directory
  contains the underscore '_' character. The script will not
  work. This is caused by a lot of character translation
  fiddling; I don't intend to try to fix it in the short term,
  do don't store files to be processed in directories containing
  an underscore character.
</p>

<h3>Automating report archiving</h3>
<p>
This was discussed in the previous section. If you provide a
parameter option <em>--archivedir=dirname</em> to process_server_details.sh 
then an archive file
of the current report processing is automatically created in
that directory for you at the end of the processing run.
</p>

<h3>Processing only a single server</h3>
<p>
If you have many servers, and only wish to perform the scan again on that
one servers data after you have made changes that is now possible, but there
are special considerations to take into account.
</p>
<p>
The parameter option <em>--oneserver=servername</em> can be used to
specify a specific server to be re-processed but it has the following
conditions due to the main index needing to be rebuilt.
</p>
<ul>
<li>any existing server results that are missing files needed to re-populate
the index will also be selected for processing automatically if its collected
datafiles are available to do so (user is prompted to continue or abort <b>so
this will impact batch jobs</b> however batch jobs are unlikely to
perform single server reruns</li>
<li>any existing server results that were created using a different version
of the processing script 
will also be selected for processing automatically if its collected
datafiles are available to do so (user is prompted to continue or abort <b>so
this will impact batch jobs</b> however batch jobs are unlikely to
perform single server reruns. This is required as the main index page
format may change and require data not provided by older versions</li>
<li>if another server does need to be reprocessed and its collected data files are
not available the rerun will be aborted to avoid the main index becoming incorrect</li>
</ul>
<p>
As a result of these conditions a request to re-process a single server after
upgrading the processing script will result in all other servers neededing to be
reprocessed as well; however if you have upgraded the processing script you should
have done this anyway.
</p>
<p>
However if you are re-processing a single server under the same version of
the processing script as all other servers were processed under then only
the single server specified will be reprocessed.
</p>

<h3>Processing only updated servers</h3>
<p>
The ability exists (since version 0.08) to request processing only for changed/updated 
server datafiles.
</p>
<p>
If you have many servers you may regularly copy new collected datafiles to the processing
server on an ad-hoc or scheduled basis, but do not want the delay of using a full processing
run of a lot of single server processing runs to process the updated files.
</p>
<p>
Since version 0.08 it is possible to use the '--checkchanged=list' and '--checkchanged=process'
options to list or process all servers with collected datafiles that identify themselves as
being snapshoted after the server was last processed.
</p>
<p>
Unlike the --oneserver option this option assumes all other servers were processed using
the current version of the processing script and will not force re-processing of servers
with obsolete results.
</p>

<H2>Overiding/Customising the default reporting</H2>
<p>
The default tight security checks may not suit your environment, to
allow for this there is the ability to override the default checks
performed by the processing script. This is done by using
customisation files, so you do not need to go to the effort
of modifying the scripts themselves.
</p>
<p>
The overrides may be set at a <em>global site level</em> or at
an <em>individual server level</em> (<b>not both</b>) depending upon your requirements.
There are two types of override files
<ul>
	<li>ALL.custom        - will override defaults for all processed servers that do not have their own custom file</li>
	<li><em>hostname</em>.custom - will override defaults for a specific hostname. This would
	be the most common as if for example you had dozens of servers but only three with mariadb
	you would not put mariadb customisations into the default file</li>
</ul>
</p>
<p>
There are only limited things that can be overridden, as your system
is either secure or not. The overrides allowed are for things that
may not necessarily make it insecure such as you wishing an application
to listen on a tcp-ip port as OK which would be a specific server
override, or a shared home directory to be considered a 'shared' directory,
such as halt and bin both having a home directory of bin so ownership
of the directory should not be halt or bin but root, and it shouldn't
be 700 or nobody could get to it.
</p>
<p>
<b>I M P O R T A N T :</b><br>
If you create a server specific customisation file be aware that
it replaces any site wide file (replaces, not merged with).
As such if you create a server
specific custom file ensure you copy any values you put in the
global site custom file into the server specific ones if required.
</p>
<p>
<b>As custom files are a topic into themselves the use of, and
values available in, the custom files are covered in the
Appendix A section</b>.
</p>
<H2>Processing Performance tips</H2>
<p>
As my list of servers has grown I have needed to find ways of processing 
servers without having to process all servers in each processing run.
</p>
<p>
The --checkchanged=process|list option was introduced to allow staggered
processing (as long as the processing versions do not change) so you can stagger
collection of datafiles from servers to only process a few each day rather than have to re-process
all servers everyday.
</p>
<p>
The --indexonly=yes option was origionally added to permit processing to be
carried out on multiple servers, with the results (and raw datafiles) being
able to be copied to a single master results server where the --indexonly=yes
option will be able to merge all the results for a consolidated view.
</p>

<hr>
<a name="sect4"></a>
<center><a href="#sect3">[Previous Section]</a> <a href="#sect0">[Index]</a> <a href="#sect5">[Next Section]</a></center>
<H1>4. Viewing the security report output</H1>
<p>
The reports are produced in html format. These are 
in the <b>results</b> directory under the application directory.
</p>
<p>
Use a web browser to open the index.html file in that directory.
The initial index file contains a scorecard summary of total alerts and
warnings for every server processed so you can quickly identify
problems, and links to each individual servers more detailed scorecards
and reports to see exactly what the alerts or warnings for each server
were caused by.
</p>

<hr>
<a name="sect5"></a>
<center><a href="#sect4">[Previous Section]</a> <a href="#sect0">[Index]</a> <a href="#sect6">[Next Section]</a></center>
<H1>5. Appendix A - Reporting Override Custom File Syntax</H1>

<H2>USING CUSTOM FILES</H2>

<H3>1. Why Custom Files</H3>
<p>
There will be occasions where the defaults used by the processing script
are not adequate or realistic for one of more servers being procesed.
To allow for that, rather than you having to edit the script, you are
permitted to create customised overide files.
</p>
<p>
There are two types of override files
<ul>
<li>ALL.custom        - will override defaults for processed servers if there is no server specific file</li>
<li><em>hostname</em>.custom - will override defaults for a specific hostname</li>
</ul>
There are only limited things that can be overridden, as your system
is either secure or not. The overrides allowed are for things that
may not necessarily make it insecure such as you wishing an application
to listen on a tcp-ip port as OK which would be a specific server
override, or a shared home directory to be considered a 'shared' directory,
such as halt and bin both having a home directory of bin so ownership
of the directory should not be halt or bin but root, and it shouldn't
be 700 or nobody could get to it.
</p>
<p>
What can be overridden using customisation files is covered in this appendix.
</p>
<p>
<b>I M P O R T A N T :</b><br>
A server specific custom file is not merged with the default
ALL.custom file, it replaces it. As such if you create a server
specific custom file ensure you copy any values you put in the
global custom file into the server specific ones if required.
<em>Use the INCLUDE_CUSTOM_RULES where possible</em> so rules
for applications can be added/removed using those rather than
having huge custom files per server.
</p>
<p>
<b>I M P O R T A N T :</b><br>
The custom files must be in the directory 'custom' in the root
of this toolkit. As shipped this is where the supplied ALL.custom
file lives so you have an example tree already setup.
The expected tree structure is...
<pre>
   /something/something/bin
   /something/something/custom           # serevr custom files
   /something/something/custom_includes  # package definitions to include
   /something/something/doc
   /something/something/results          # html results
</pre>
...all script references to custom files are relative to the bin
directory (../custom/*) so please keep this directory structure.
</p>
<p>
<b>I M P O R T A N T :</b><br>
Any line with # in column 1 is a comment line, <em>do not put comments
at the end of a data line</em>.
</p>

<H3>2. Network Port Overrides</H3>
<p>
TCP-IP ports are a gateway to your system. You should know
what is listening on them. <em>If a port is not defined in the custom file
it will be considered an unexpected open port and will be considered critical</em>.
</p>
<p>
In the reports, even if a TCP-IP port is allowed <em>you will
still get a warning be default if the port is listening on all
interfaces</em>, you can only get an OK when it is more securely bound to
a specific interface or address.
</p>
<p>
Overrides are available for tcp, tcp6, udp, udp4, raw and raw6 ports.
An override must be correct for the protocol type, an allow line for a tcp6 port
will not affect alerting behavior on any other port type, this allows for the unlikely case
where one application may use tcp port NNN and another application use tcp6 port NNN.
</p>
<p>
It is <em>important to note</em> that these port settings are also used when checking
firewall rules, if a firewall port is open but no entry of one of the below exists in the custom
file an alert will also be raised in the firewall checks for an unexpected port open.
</p>
<p>
The syntax of network port overrides are (where &ltversion&gt is either 4 or 6) line is...<br>
<span style="background-color: #FFCCFF"><b>TCP_PORTV&ltversion&gt_ALLOWED=:<em>portnum</em>:<em>description[:WILD]</em></b></span><br />
<span style="background-color: #FFCCFF"><b>UDP_PORTV&ltversion&gt_ALLOWED=:<em>portnum</em>:<em>description[:WILD]</em></b></span><br />
<span style="background-color: #FFCCFF"><b>RAW_PORTV&ltversion&gt_ALLOWED=:<em>portnum</em>:<em>description[:WILD]</em></b></span><br />
<br />
For example...<br>
<span style="background-color: #D7FFFF">TCP_PORTV4_ALLOWED=:22:ssh</span>
<span style="background-color: #D7FFFF">TCP_PORTV6_ALLOWED=:9090:cockpit</span>
</p>
<p>
If the optional :WILD is appended after the description it will be considered OK
for the specified port to be listening on all interfaces (on 0.0.0.0 or :::),
<b>but this should be used only if your site has a specific reason for an application
to listen on all interfaces</b>, you should configure your application to only listen
on the interfaces it needs rather than on all interfaces.
</p>
<p>
As mentioned above these port values are also checked against firewall rules, so
not directly relevant but worth mentioning is that if you are not actually using
iptables or nft (or firewalld) then to avoid an alert saying no firewall is installed
you should also add in the custom file<br />
<span style="background-color: #D7FFFF">NO_FIREWALL_INSTALLED=YES</span>
</p>

<H3>3. Network Overrides by process</H3>
<p>
Unfortunately some applications use randomly assigned network ports
and as such cannot be configured using the explicit port paramaters above.
An example of such a process would be rpcbind.
</p>
<p>
In order to allow you to downgrade these from critical alerts
by defining the processes that can use random ports.
</p>
<p>
Using these parameters will still result in warnings if the
port is listening on all interfaces as that is insecure, and there
is no ability provided to permit listening on all interfaces to
be permitted using this method. If the port is listening on specific
interfaces no alerts will be raised but the report will show
alerts suppressed this way in a different colour to indicate they are
still considered as insecure.
</p>
<p>
The value passed in this parameter must be the full details of the running
process as reported by 'ps -ef' as we want as exact match as possible.
<p>
The syntax of network port overrides by process name are (where &ltversion&gt is either 4 or 6) line is...<br>
<span style="background-color: #FFCCFF"><b>NETWORK_TCPV&ltversion&gt_PROCESS_ALLOW=/program/name and parameters</b></span><br />
<span style="background-color: #FFCCFF"><b>NETWORK_UDPV&ltversion&gt_PROCESS_ALLOW=/program/name and parameters</b></span><br />
<span style="background-color: #FFCCFF"><b>NETWORK_RRAW&ltversion&gt_PROCESS_ALLOW=/program/name and parameters</b></span><br />
<br />
Examples:<br />
<span style="background-color: #D7FFFF">NETWORK_UDPV4_PROCESS_ALLOW=avahi-daemon: running [phoenix.local]</span><br />
<span style="background-color: #D7FFFF">NETWORK_UDPV6_PROCESS_ALLOW=avahi-daemon: running [phoenix.local]</span><br />
<span style="background-color: #D7FFFF">NETWORK_UDPV4_PROCESS_ALLOW=/sbin/rpcbind -w</span><br />
<span style="background-color: #D7FFFF">NETWORK_UDPV6_PROCESS_ALLOW=/sbin/rpcbind -w</span><br />
</p>

<H3>4. Home Directory Ownership Overrides</H3>
<p>
This override is promarily for home directory checks. As
you are aware many of the system users share a home 
directory, and by default the report alerts on any home
directory not owned by the correct user.<br>
<b>This override is only for system directories</b>, it will
enable you to to flag as OK a directory that is owned by
root rather than the expected owner.
</p>
<p>
The syntax of a directory ownership override is...<br>
<span style="background-color: #FFCCFF"><b>ALLOW_OWNER_ROOT=<em>dirname:</em></b><br><br></span>

For example...<br>
<span style="background-color: #D7FFFF"><b>ALLOW_OWNER_ROOT=bin:</b><br></span>
(You cannot provide any leading / of leading directory path,
the dirname is the basename of the directory being checked.
<b>be carefull</b> in it's use as it will treat /bin and /usr/tmp/bin
for example with the same rule and allow the root owner (which is
probably OK for system directories)).
</p>
<p>
The trailing : is mandatory.
</p>
<p>
<em>Note</em>: home directories must be owned by the user defined
in the /etc/passwd file as having the home directory, or owned
by root in the case of shared directories (as defined by using
this override).
Anything else is a security
concern so cannot be overridden.
</p>

<H3>5. Home Directory Permission Overrides</H3>
<p>
This is intended primarily for home directory checks.
Any home directory not secured 700 or tighter (d***------)
is a security risk as home directories should be secure.
</p>

<H4>5.1 General System Directory Overrides</H4>
<p>
System home directories however have a requirement that
other users can get into them, to run programs and access
configuration files.
</p>
<p>
To allow for this during the checks it is possible to flag
a user home directory directory as a system directory, in which case any of the
permissions
drwxr-xr-x, drwxr-x--x, drwx--x--x, dr-xr-xr-x or dr-xr-x---
will be acceptable for it <b>if it is owned by a system user</b>.
</p>
<p>
To request a directory be treated as a system directory the
format of the custom file entry is...<br>
<span style="background-color: #FFCCFF"><b>ALLOW_DIRPERM_SYSTEM=<em>dirname:</em></b><br><br></span>

For example...<br>
<span style="background-color: #D7FFFF"><b>ALLOW_DIRPERM_SYSTEM=bin:</b></span>
</p>
<p>
The trailing : is mandatory.
</p>

<p>
Occasionally you may install software packages under a different userid to allow them
to run as a non-root userid, in which case all files should be owned by that non-root
user. To avoid those being reported as being owned by a non-system user you can in the
custom file for the server define additional system file owners.
For example to allow jetty to own system files...<br>
<span style="background-color: #D7FFFF"><b>ADD_SYSTEM_FILE_OWNER=jetty</b></span>
</p>


<H4>5.2 Explicit Directory Overrides</H4>
<p>
There are some directories (very few) that just cannot fit
into the general system category. I have only found one
(mail-which has the sticky bit set for mail group).
As a general rule thats OK for a directory, so this override
option is specifically for directory overrides.
</p>
<p>
This override should be used with caution as it is used as a catch-all
of last resort for directories not meeting all other rules,
and is done by directory name (not full path) so will match
all directories of the name.
</p>
<p>
To force a directory override to use an explicit permission
value (which will only be checked if all other checks fail)
use the custom file entry...<br>
<span style="background-color: #FFCCFF"><b>ALLOW_DIRPERM_EXPLICIT=<em>dirname fullperms</em></b><br><br></span>

For example...<br>
<span style="background-color: #D7FFFF"><b>ALLOW_DIRPERM_EXPLICIT=mail drwxrws--x</b></span>
</p>

<H3>6. System File Checks</H3>

<H4>6.1 Forcing a system file into OK state</H4>
<p>
There shouldn't be any files you need to do this for, but
unfortunately there will always be exceptions, for example
/usr/games/Maelstrom/Maelstrom-Scores on my development
server needs world write access as I don't want users to be
root to run a game (yes I allow games). Another seems to
be /var/spool/at/.SEQ although this only appears on one
of my servers (I don't use at, but obviously have at some
point).
</p>
<p>
To stop the report constantly throwing up alerts for
badly secured system files you know is badly secured,
but wish to keep the way it is; you can add an entry to
the custom file to force the file to be treated as OK.
</p>
<p>
To do so use the FORCE_PERM_OK entry with the filename,
<b>including the full path</b>, a colon, and the expected permissions as the value. The game score file
mentioned above is used in this example...<br>

<span style="background-color: #D7FFFF"><b>FORCE_PERM_OK=/usr/games/Maelstrom/Maelstrom-Scores:-rw-rx-rx-</b></span>
</p>

<p>
<b>And for bad owners</b> I have also added a FORCE_OWNER_OK tag, with my upgrade from
FC3 to FC6 a single system file was owned by vcsa rather than a system user, for one
file I was not willing to add vcsa to the system file owner list so create the
additional tag (which alas really slows the checking down some more as every badly owned
file now does a check for the override (actually, doesn't slow much on my system
as I only have that one file, time to clean up yours :-) ).<br>
<span style="background-color: #D7FFFF"><b>FORCE_OWNER_OK=/usr/libexec/mc/cons.saver:vcsa</b></span>
</p>


<H4>6.2 Managing "sloppy" /var checking</H4>
<p>
As the /var filesystem can, and often is, used by every
user (/var/tmp anyway) you will often find many files under
/var secured incorrectly.
</p>
<p>
It is possible to handle these in the report with the configuration file
values below (note: WARN is the default).
<pre><b><span style="background-color: #FFCCFF">
  ALLOW_SLOPPY_VAR=OK&nbsp&nbsp&nbsp&nbsp
  ALLOW_SLOPPY_VAR=WARN&nbsp&nbsp
  ALLOW_SLOPPY_VAR=NO&nbsp&nbsp&nbsp&nbsp
</span></b></pre>
</p>
<p>
The actions taken are (which may be overridden for some files if other options affecting /var are used)<br />
<b>OK</b> - report only a summary line of the total count of badly owned and badly secured files found<br />
<b>WARN</b> - report the details of each insecure file found, report them as warnings<br />
<b>NO</b> - report the details of each insecure file found, report them as alerts
</p>
<p>
By default the setting is WARN so you are aware of all the issues but
don't have a horrendously high critical alert total.
</p>
<p>
It should also be noted the following parameter can be used to suppress many of the alerts also.
</p>

<H4>6.3 Permitting files under /var to be group writeable</H4>
<p>
The default umask on most systems is for new files created to
be writeable by owner and group. This is normally not an issue other
than this security checking toolkit alerting on such files under /var,
files created by users such as mysql, apache, clamav, bacula can cause
this toolkit to generate hundreds of alerts when it checks files under /var.
</p>
<p>
This option will consider it valid for files to be group writeable 
as long as
</p>
<ul>
<li>they are under the /var directory path</li>
<li>the group matches the owner of the file (for example user mysql and group mysql)</li>
</ul>
<pre><b><span style="background-color: #FFCCFF">
ALLOW_VAR_FILE_GROUPWRITE=YES
</span></b></pre>
<p>
If any value other than YES is provided then NO is assumed.
</p>

<H3>7. Turning off warnings for manual check items</H3>
<p>
There are some checks that always by default raise a
warning alert, as these require manual checking. The
warning alert is raised to highlight that a manual
check must be performed to ensure that eveything
is OK.
</p>
<p>
It is possible to override two of these, the two
most unlikely to be a security risk. These are the
<ul>
		<li>warning if a custom file was used and should be reviewed</li>
		<li>warning to manually check log file retentions</li>
</ul>
</p>
<p>
These can be turned of by including in the custom
file used for a server the entries below<br>
<pre><b><span style="background-color: #FFCCFF">
  NOWARN_ON_CUSTOMFILE=YES&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
  NOWARN_ON_MANUALLOGCHECK=YES&nbsp&nbsp
</span></b></pre>
</p>
<p>
It is probably inadvisable to use the overrides to turn these off as
manual reviews should be performed, but as I
personally are happy with my settings I use these
to keep the results boxes green rather than yellow.
</p>

<H3>8. Turning off alerts for known/ok SUID files</H3>
<p>
Files with the <b>suid</b> bits set are possible security risks,
however most servers require some files with these bits set to
function (ksu, suexec etc need them set). To avoid these
raising alerts each time you run the check you have the option of
specifying files that are permitted to have suid bits set for each
server.</p>
</p>
<p>
These can be turned of by including in the custom
file used for a server the entry below<br>
<pre><b><span style="background-color: #FFCCFF">
  SUID_ALLOW=/path/.../filename:
</span></b></pre>
</p>
<p>
Note that in this custom file entry the <b>full path and filename must be provided</b>
rather than just the filename part, for obvious reasons;
don't want users having their own versions of programs that need suid bits set.
</p>
<p>
The trailing : is mandatory.
</p>
<p>
It is also important to note that any SUID_ALLOW entries in a custom file that refer to files
that no longer exist on the system being checked will generate warnings requesting you to
remove obsolete entries from the custom file.
</p>

<hr>
<a name="sect6"></a>
<center><a href="#sect5">[Previous Section]</a> <a href="#sect0">[Index]</a> <a href="#sect7">[Next Section]</a></center>
<H1>6. Appendix B - Customisation File settings available</H1>

<p>
Sample customisation files are supplied in the tarball that contains the application.
These are in the directory <em>custom</em>.
</p>

<p>
Filenames must be named <b><em>servername</em>.custom</b> if you create custom files on a per-server basis.
There should alway be a ALL.custom file to provide defaults for all servers that do not have a specific
custom file although that is also optional.
</p>
<p>
The search for customisations is
</p>
<ol>
<li>if a servername.custom exists for a servername use that</li>
<li>if no servername.custom exists for a server use the default ALL.custom if it exists</li>
<li>if no customisation file exists the extremely tight processing defaults will be used</li>
</ol>
<p>
It is <b>important to note</b> custom files are not merged, if you have a servername.custom 
file everything in ALL,custom will be ignored. This is a deliberate design decision, your
ALL.custom may accomodate most of your servers but if you have one you want more tightly
secured the last thing you would want is less strict settings fom ALL.custom being merged with
you servername.custom settings.
</p>
<p>
The list of parameters available <b>is not in alpabetical order</b>, but quite jumbled.
I will get around to updating that one day. In the meantime things are added at the end
as I implement them.
</p>

<table border="1">
<tr><td>TCP_PORTV4_ALLOWED=:<em>port:description[:WILD]</em></td><td>
Used to specify a TCPV4 port that is expected to be opened on the server.
There must be a : before and after the port number, anything after the second : is a
free form description to describe the use of the port although the description
cannot contain the : character.<br>
Examples<br>
TCP_PORTV4_ALLOWED=:22:ssh server<br />
<br />
Note that this will stop a port being raised as a critical issue in the report, but
if the port is listening on all interfaces it will still be raised as a warning
unless the optional :WILD parameter is appended to indicate it can listen on
all interfaces.
<br />
The optional :WILD option must only be used if you have a site specific reason
for an application to listen on all interfaces, it is prefered that the application
be configured in a more secure manner. If used while it will not be treated as
an alert the report will highlight it in a different color to indicate that it
is considered insecure.
</td></tr>
<tr><td>TCP_PORTV6_ALLOWED=:<em>port:description[:WILD]</em></td><td>
Used to specify a TCPV6 port that is expected to be opened on the server.
There must be a : before and after the port number, anything after the second : is a
free form description to describe the use of the port although the description
cannot contain the : character.<br>
Examples<br>
TCP_PORTV6_ALLOWED=:9090:Cockpit
<br />
Note that this will stop a port being raised as a critical issue in the report, but
if the port is listening on all interfaces it will still be raised as a warning
unless the optional :WILD parameter is appended to indicate it can listen on
all interfaces.
<br />
The optional :WILD option must only be used if you have a site specific reason
for an application to listen on all interfaces, it is prefered that the application
be configured in a more secure manner. If used while it will not be treated as
an alert the report will highlight it in a different color to indicate that it
is considered insecure.
</td></tr>
<tr><td>UDP_PORTV4_ALLOWED=:<em>port:description[:WILD]</em></td><td>
Used to specify a UDPV4 port that is expected to be opened on the server.
There must be a : before and after the port number, anything after the second : is a
free form description to describe the use of the port although the description
cannot contain the : character.<br>
Examples<br>
UDP_PORTV4_ALLOWED=:53:dnsmasq
<br />
Note that this will stop a port being raised as a critical issue in the report, but
if the port is listening on all interfaces it will still be raised as a warning
unless the optional :WILD parameter is appended to indicate it can listen on
all interfaces.
<br />
The optional :WILD option must only be used if you have a site specific reason
for an application to listen on all interfaces, it is prefered that the application
be configured in a more secure manner. If used while it will not be treated as
an alert the report will highlight it in a different color to indicate that it
is considered insecure.
</td></tr>
<tr><td>UDP_PORTV6_ALLOWED=:<em>port:description[:WILD]</em></td><td>
Used to specify a UDPV6 port that is expected to be opened on the server.
There must be a : before and after the port number, anything after the second : is a
free form description to describe the use of the port although the description
cannot contain the : character.<br>
Examples<br>
UDP_PORTV6_ALLOWED=:53:dnsmasq
<br />
Note that this will stop a port being raised as a critical issue in the report, but
if the port is listening on all interfaces it will still be raised as a warning
unless the optional :WILD parameter is appended to indicate it can listen on
all interfaces.
<br />
The optional :WILD option must only be used if you have a site specific reason
for an application to listen on all interfaces, it is prefered that the application
be configured in a more secure manner. If used while it will not be treated as
an alert the report will highlight it in a different color to indicate that it
is considered insecure.
</td></tr>
<tr><td>NETWORK_TCPV4_PROCESS_ALLOW=<em>full process details as shown by 'ps'</em></td><td>
This parameter can be used in the case where a known process is permitted to
use a TCPV4 port but the port cannot be explicitly defined using the above entries
because the process uses random port numbers.<br />
While if used it will not be treated as
an critical alert, but if it listens on a specific interface the report will highlight it
in a different color to indicate that it is considered insecure.
<em>If the port listens on all interfaces it will still raise a warning alert</em>.
</td></tr>
<tr><td>NETWORK_TCPV6_PROCESS_ALLOW=<em>full process details as shown by 'ps'</em></td><td>
This parameter can be used in the case where a known process is permitted to
use a TCPV6 port but the port cannot be explicitly defined using the above entries
because the process uses random port numbers.<br />
While if used it will not be treated as
an critical alert, but if it listens on a specific interface the report will highlight it
in a different color to indicate that it is considered insecure.
<em>If the port listens on all interfaces it will still raise a warning alert</em>.
</td></tr>
<tr><td>NETWORK_UDPV4_PROCESS_ALLOW=<em>full process details as shown by 'ps'</em></td><td>
This parameter can be used in the case where a known process is permitted to
use a UDPV4 port but the port cannot be explicitly defined using the above entries
because the process uses random port numbers.<br />
Example<br />
NETWORK_UDPV4_PROCESS_ALLOW=/usr/bin/rpcbind -w -f
<br />
While if used it will not be treated as
an critical alert, but if it listens on a specific interface the report will highlight it
in a different color to indicate that it is considered insecure.
<em>If the port listens on all interfaces it will still raise a warning alert</em>.
</td></tr>
<tr><td>NETWORK_UDPV6_PROCESS_ALLOW=<em>full process details as shown by 'ps'</em></td><td>
This parameter can be used in the case where a known process is permitted to
use a UDPV6 port but the port cannot be explicitly defined using the above entries
because the process uses random port numbers.<br />
Example<br />
NETWORK_UDPV6_PROCESS_ALLOW=/usr/bin/rpcbind -w -f
<br />
While if used it will not be treated as
an critical alert, but if it listens on a specific interface the report will highlight it
in a different color to indicate that it is considered insecure.
<em>If the port listens on all interfaces it will still raise a warning alert</em>.
</td></tr>
<tr><td>NETWORK_RAWV4_PROCESS_ALLOW=<em>full process details as shown by 'ps'</em></td><td>
This parameter can be used in the case where a known process is permitted to
use a RAWV4 port but the port cannot be explicitly defined using the above entries
because the process uses random port numbers.<br />
Example<br />
NETWORK_RAWV4_PROCESS_ALLOW=/usr/bin/rpcbind -w -f
<br />
While if used it will not be treated as
an critical alert, but if it listens on a specific interface the report will highlight it
in a different color to indicate that it is considered insecure.
<em>If the port listens on all interfaces it will still raise a warning alert</em>.
</td></tr>
<tr><td>NETWORK_RAWV6_PROCESS_ALLOW=<em>full process details as shown by 'ps'</em></td><td>
This parameter can be used in the case where a known process is permitted to
use a RAWV6 port but the port cannot be explicitly defined using the above entries
because the process uses random port numbers.<br />
Example<br />
NETWORK_RAWV6_PROCESS_ALLOW=/usr/bin/rpcbind -w -f
<br />
While if used it will not be treated as
an critical alert, but if it listens on a specific interface the report will highlight it
in a different color to indicate that it is considered insecure.
<em>If the port listens on all interfaces it will still raise a warning alert</em>.
</td></tr>
<tr><td>TCP_OUTBOUND_SUPPRESS=:port:description</td><td>
<em>This is used only in the firewall rule checking section and only if there
is no matching port open on the server</em>. Depending on how
paranoid your server firewall rules are there will be accept rules for
outbount traffic (where no port is open on the local server as the target
is a remote server or network). This parameter is used to suppress an
alert for a firewall accept rule for a TCP port that is not open on the local
server.<br />
Example<br />
TCP_OUTBOUND_SUPPRESS=:123:local network ntp servers
</td></tr>
<tr><td>UDP_OUTBOUND_SUPPRESS=:port:description</td><td>
<em>This is used only in the firewall rule checking section and only if there
is no matching port open on the server</em>. Depending on how
paranoid your server firewall rules are there will be accept rules for
outbount traffic (where no port is open on the local server as the target
is a remote server or network). This parameter is used to suppress an
alert for a firewall accept rule for a UDP port that is not open on the local
server.<br />
Example<br />
UDP_OUTBOUND_SUPPRESS=:123:local network ntp servers
</td></tr>
<tr><td>TCP_NETWORKMANAGER_FIREWALL_DOWNGRADE=:port:<br />
UDP_NETWORKMANAGER_FIREWALL_DOWNGRADE=:port:</td><td>
NetworkManager or Firewalls opens firewall ports for services that have not necessarily
been manually configured by the site administrator. This parameter will
allow downgrading of an alert to a warning if the following conditions
are met; the port is not configured as an expected allowed port,
nothing is listening on the port <b>and NetworkManager or Firewalld are running</b>
on the server. This requirement is needed as I have servers not running firewalld where
NetworkManager may (all I can think of) add ports to iptables that are not defined in any
manually configured iptables rulesets, and on servers not running NetworkManager but using
Firewalld where ports are opened in the firewall that are not defined to firewalld by the system administrator
(they do not show in firewall-cmd queries on services or ports).<br />
<b>Before using this parameter on firewalld systems ensure you have
checked all services and ports defined to firewalld and remove them from there
if possible rather than downgrading the alert</b>.
</td></tr>
<tr><td>NETWORK_PORT_NOLISTENER_TCPV4_OK=port:optional description<br />
NETWORK_PORT_NOLISTENER_TCPV6_OK=port:optional description<br />
NETWORK_PORT_NOLISTENER_UDPV4_OK=port:optional description<br />
NETWORK_PORT_NOLISTENER_UDPV6_OK=port:optional description<br />
NETWORK_PORT_NOLISTENER_RAWV4_OK=port:optional description<br />
NETWORK_PORT_NOLISTENER_RAWV6_OK=port:optional description<br />
</td><td>
This parameter is used to downgrade alerts to warnings for ports that are
expected to be listening and have firewall ports open for them but
at the time the data was collected had no application listening on the port.
This is useful for ports such as the X11 ssh forwarding port that is only
active when a user has a ssh session to the host, and for vnc consoles for
VMs where the VMs are not always running on the default host.<br />
It is still raised as a warning, as ifd the port is open in the firewall
any rouge user application can use the port which is undesirable.
<br />And yes the checks do treat Tcp+Udp V4 and V6 as distinct entries.
<br /><b>Important to note the : after the port is mandatory</b> even if you do not
add a description.
</td></tr>
<tr><td>ALLOW_OWNER_ROOT=<em>directoryname:</em></td><td>
Allows a directory that is expected to be owned by a user other than root to
be accepted as OK for reporting if the directory is owned by the root user.
This is needed as many system users have home directories of /bin, /sbin or /root
(for example on Fedora31 users operator/shutdown/halt have a home directory of /root
but the directory needs to be owned bu root not by users operator/shutdown/halt).
<b>It is important to note that the override will apply to any directory matching
the name so use with caution</b> (ie: if =bin is used then /bin, /usr/bin,
/home/fred/bin, /rootkit/hacker/bin
will be accepted as OK to be owned by root if not owned by the default owner).<br />
Examples<br />
ALLOW_OWNER_ROOT=bin:<br />
ALLOW_OWNER_ROOT=sbin:<br />
ALLOW_OWNER_ROOT=root:
</td></tr>
<tr><td>ALLOW_DIRPERM_SYSTEM=<em>directoryname:</em></td><td>
This can be used to override alerts for <b>user home directories</b> that need to have shared
(read) access, obviously direcories such as /bin and /sbin that are configured as some user home
directories but need to permit other users to traverse down them so
this configuration option can be used for <b>user home directories </b> directories such as those; user directories serving
'public_html' pages is another example that would need this override. It will allow
directories secured as drwxr-xr-x, drwxr-x--x, drwx--x--x, dr-xr-xr-x, dr-xr-x--- or drwxr-x---
to be accepted as OK.<br />
Examples<br />
ALLOW_DIRPERM_SYSTEM=bin:<br />
ALLOW_DIRPERM_SYSTEM=sbin:<br />
ALLOW_DIRPERM_SYSTEM=adm:<br />
<b>It is important to note that the override will apply to any directory matching
the name so use with caution</b> (ie: if =bin is used then /bin, /usr/bin,
/home/fred/bin, /rootkit/hacker/bin
will be accepted as OK if they are secured with any of the three masks mentioned above.<br />
</td></tr>
<tr><td>ALLOW_DIRPERM_EXPLICIT=<em>directoryname permissions</em></td><td>
This is used to explicitly define permissions that can be set on a directory if the
existing permissions would be considered a critical condition. There are some system directories
and home directories that will be secured differently to recomended settings and these can be set
here. <b>The last two example entries are for Fedora systems where systrem user account home
directories are to symbolic links instead of a real directory</b><br />
Examples<br />
ALLOW_DIRPERM_EXPLICIT=mail drwxrwxr-x<br />
ALLOW_DIRPERM_EXPLICIT=named drwxr-x---<br />
ALLOW_DIRPERM_EXPLICIT=gdm drwxrwx--T<br />
ALLOW_DIRPERM_EXPLICIT=sshd drwx--x--x<br />
ALLOW_DIRPERM_EXPLICIT=avahi-autoipd drwxrwx--T<br />
ALLOW_DIRPERM_EXPLICIT=bin lrwxrwxrwx<br />
ALLOW_DIRPERM_EXPLICIT=sbin lrwxrwxrwx<br />
<b>It is important to note that the override will apply to any directory matching
the name so use with caution</b> (ie: if =bin is used then /bin, /usr/bin,
/home/fred/bin, /rootkit/hacker/bin
will be accepted as OK if they match the directory permissions specified.<br />
</td></tr>
<tr><td>FORCE_PERM_OK=<em>fullpathandfilename:expectedperms</em></td><td>
Ocassionally there will be a file that alerts that does not fit any generic rules,
this custom file setting can be used to force any checks against the file <b>permissions</b>
of a file to be considered OK reguardless of what the file permissions actually are.
Examples<br />
FORCE_PERM_OK=/usr/libexec/mc/cons.saver:-rw-rw-rw-<br />
Unlike other overrides this is specific to one file and the full path and filename
must be specified, plus the expected permissions seperated by a colon. It is expected there would be very few of these entries in a
configuration file (I currentrly have zero).
</td></tr>
<tr><td>FORCE_ANYFILE_OK=<em>filename fileperms:</em></td><td>
Allow a file of this name to be forced OK <em>under any directory</em> if
it fails initial default checks.
A risk as this check will be performed on any file matching the
basename of the full file path, but Fedora now generates lots of dynamic PCI bus entries
under the /sys/devices/pciNNNN:NN path as --w--w----. and we do not
want them generating false alerts.
Examples<br />
FORCE_ANYFILE_OK=remove --w--w----:<br />
FORCE_ANYFILE_OK=rescan --w--w----:<br />
The risk is minimised by the override requiring the file permissions that
are expected to be explicitly provided. <em>Also</em> this parameter is only
used if the filename being checked has already failed all previous checks,
this is the last check in the chain.
</td></tr>
<tr><td>FORCE_OWNER_OK=<em>fullpathandfilename:expectedowner</em></td><td>
Ocassionally there will be a file that alerts that does not fit any generic rules,
this custom file setting can be used to force any checks against the file <b>owner</b>
of a file to be considered OK as long as the owner of the file actually matches the expectedowner provided.<br />
Examples<br />
FORCE_OWNER_OK=/usr/libexec/mc/cons.saver:vcsa<br />
Unlike other overrides this is specific to one file and the full path and filename
must be specified, it is expected there would be very few of these entries in a
configuration file (I currentrly have zero).
</td></tr>
<tr><td>SUID_ALLOW=<em>fullpathandfilename:</em></td><td>
There will always be setuid files on a *nix system, and you should keep track of them.
This setting is used to define every setuid file you expect to exist on the server 
being checked. Any setuid file on the server not defined by these entries will
be reported as a critical issue.<br />
Examples<br />
SUID_ALLOW=/usr/sbin/sendmail.sendmail:<br />
SUID_ALLOW=/usr/sbin/userhelper:<br />
SUID_ALLOW=/usr/bin/lockfile:<br />
SUID_ALLOW=/usr/bin/sudo:<br />
SUID_ALLOW=/usr/bin/crontab:<br />
Unlike other overrides this is specific to one file and the full path and filename
must be specified. <em>It should also be noted that the report will alert on any
entries defined this way where the file does not exist on the server to ensure
you do not leave stale entries in this list</em>.
</td></tr>
<tr><td>SUID_SUPPRESS_DOCKER_OVERLAYS=YES</td><td>
On servers running docker containers a lot of SUID files used by the containers
are placed in directories under /var/lib/docker/overlay2/<em>containerid</em>.
As contaner ids are generated there is no way to hard code allowed filenames using
the above parameter; so this parameter can be used to suppress all alerts for
for SUID file if they are under the directory path /var/lib/docker/overlay2.
If this parameter is used the file security page of the report will list the
number of alerts suppressed for this reason and provide a link to all the files
that had alerts suppressed. <b>You</b> are responsible for reviewing this list
of suppressed files if you suppress the alerts.
</td></tr>
<tr><td>SUID_SUPPRESS_SNAP_OVERLAYS=YES</td><td>
On Ubuntu servers (at least from release 20.04 onward) many applications are installed
as SNAP packages, these contain copies of filesystem files including SUID set files
under directories such as /snap/snapd, /snap/core, /snap.core18, /snap/core*)
which will alert as unexpected SUID files, as they are in pretty much completely
randonly names directories under the paths mentioned they cannot be coded in custom
files as expected SUID files. This parameter can be used to suppress individual
alerts for those files and instead generate a single alert.
If this parameter is used the file security page of the report will list the
number of alerts suppressed for this reason and provide a link to all the files
that had alerts suppressed. <b>You</b> are responsible for reviewing this list
of suppressed files if you suppress the alerts.<br />
SNAP files are dangerous, for example '/snap/core18/1885/bin/su -' works perfectly
well even if the system /bin/su command has been locked down; and users can
install SNAP packages into their personal directories as well widening the hole;
so this parameter will suppress 100s of alerts but always raise one as SNAP packages
are dangerous.
</td></tr>
<tr><td>ADD_SYSTEM_FILE_OWNER=<em>username</em></td><td>
You will occasionally install 3rd party packages that you want to run under a user
other than root, in which case as good practise you would chown all files in that
package to the new owner. To avoid the report producing critical alerts for those
products you may install in system directories such as /usr/local or /opt it is
possible to use the configuration file to define additional users that are permitted
to own system files.<br />
Examples<br />
ADD_SYSTEM_FILE_OWNER=jetty<br />
ADD_SYSTEM_FILE_OWNER=snort<br />
ADD_SYSTEM_FILE_OWNER=logcheck<br />
On the file permissions check page of the report all 'system file owners' are listed
for the server being checked so you can keep an eye on additional entries.
</td></tr>
<tr><td>ADD_WEBSERVER_FILE_OWNER=<em>username</em></td><td>
This is only used if the server data file was produced using the collector --webpathlist option
to specifically identify web server directories that must only contain read-only files.
It allows the specification of users that are expected to be able to own those files.
<br />
Examples<br />
ADD_WEBSERVER_FILE_OWNER=apache<br />
ADD_WEBSERVER_FILE_OWNER=jetty<br />
If no entry exists and there are webserver files to be checked a default
owner of apache will be the default.
</td></tr>
<tr><td>WEBSERVER_FILE_ALLOW_WRITE_SUFFIX=<em>.suffix:</em></td><td>
This is only used if the server data file was produced using the collector --webpathlist option
to specifically identify web server directories that must only contain read-only files.<br />
The parameter can be used to identify file suffixes than can be writeable under that 
directory path.
Example<br />
WEBSERVER_FILE_ALLOW_WRITE_SUFFIX=.log:<br />
This should ideally never be required, writeable files should be kept in
directory paths outside the webserver root directories.
</td></tr>
<tr><td>WEBSERVER_FILE_ALLOW_WRITE_EXACT=<em>/full/path/file:</em></td><td>
This is only used if the server data file was produced using the collector --webpathlist option
to specifically identify web server directories that must only contain read-only files.<br />
Can be used to suppress an alert for a specific webserver file by explicitly naming
a file that can be writeable. The trailing : is required to prevent false matches such
as xxx.log matching xxx.log.1
</td></tr>
<tr><td>ALLOW_SLOPPY_VAR=WARN</td><td>
Setting this alters the report to
only warn for /var file security failures, this cuts out a lot
of critical alerts as a lot of stuff in /var is owned by many different 
userids.
</td></tr>
<tr><td>ALLOW_VAR_FILE_GROUPWRITE=YES</td><td>
A lot of files under /var tend to be group writeable with the default umasks
set with operating system installs,
and in many cases there may be a need for it. Setting this value will make the
checks consider files in directories under /var which are group writeable
as secure
<em>as long as the group matches the user</em> (ie: user mysql, group mysql)
and the 'other' permissions are not writeable.
</td></tr>
<tr><td>NOWARN_ON_MANUALLOGCHECK=YES</td><td>
There are a series of checks that have been identified as being needed to be 
performed manually. Warnings would normally be raised in the report for these
but using this configuration file option will suppress those alerts in the report.
</td></tr>
<tr><td>NOWARN_ON_CUSTOMFILE=YES</td><td>
By default is that if using a custom file to override the defaults a warning is issued.
This custom file setting suppresses that waring.<br />
As custom files are now the norm and a link from the main page of each
servers results has a link to the custom file anyway this is depeciated
and will probably be removed in the next release.
</td></tr>
<tr><td>REFRESH_INTERVAL_EXPECTED=days</td><td>
By default the main index page displays a highlighted (alert colour) for the
snapshot date field if the collected data for the snapshot is over 14 days old.
This parameter allows you to override that default with a different number
of days. For example if you have VMs that are only started as needed, or remote laptops
that only connect to the local network occasionally you may want to increase the number
of days before a data collection snapshot is considered obsolete.
</td></tr>
<tr><td>EXACT_ALERT_REASON=description text</td><td>
Occasionally there will be alerts raised you just cannot fix, this parameter
can be used multiple times to document each expected alert.
This parameter is designed to acknowledge that fact and provide a entries for which
the count of alerts displayed on the main index menu next to the server entry are
considered 'expected' and if the number of actual alerts are equal to the number
of alerts expected the total will show in green text with a (C) indicating a custom
file override. <em>Note this is the exact number of alerts expected not a less or
equal check</em> as if the alert count drops from the expected to something has changed
and you need to investigate.
<br />This should be used as a last resort as ideally you would correct all issues,
but unfortunately if the toolkit is run on debian based systems currently there
will always be at least one alert.<br />
<b>It must exactly match the alert text raised</b>, and the possible exact matches
will be shown in the expected alert report also.<br />
<b>This parameter will be ignored if there are more that 30 alerts for a server</b>
as (a) index processing time woul be too great, and (b) you should fix your problems
rather than suppress them; this parameter is to be used when you have only a couple
of alerts you require or do not intend to fix
</td></tr>
<tr><td>EXACT_ALERT_REASON_NOTES=freeform text</td><td>
This can be used muktiple times in a custom file to provide notes on why an
alert is expected rather than being fixed
</td></tr>
<tr><td>HOMEDIR_MISSING_OK=userid:</td><td>
On Fedora and CentOS (and presumably Redhat) operating systems a lot of users are
created with non-existant home directories; for example cockpit-ws and cockpit-wsinstance
user have a home directory of '/nonexisting', tss has a home directory of '/dev/null',
and there are many other examples. Each user that has a home directory that does not
exist generates an increment to the warning count by default.<br />
Using this parameter will suppress the warning count being incremented
This suppressdion is done on a per user basis so new users/packages added can be 
easily identified. The entry will still be listed in the report but in a non-warning
colour and no warning counter incremented.<br />
From V0.23 onward users with home directories of /nonexisting will not be considered
an issue as that is by design in some distros now so considered OK; so this setting
does not need to be used for those.
</td></tr>
<tr><td>SSHD_SUBSYSTEM_ALLOW=subsysname:fullsubsyscommand:</td><td>
Example SSHD_SUBSYSTEM_ALLOW=sftp:/usr/libexec/openssh/sftp-server:<br />
This parameter was added for servers that are managed by ansible, and as such
reguire sftp to be enabled. It can however be used for any subsystem enabled
by the sshd_config file. If used the alert raised for the specified subsystem
is downgraded to a warning.
</td></tr>
<tr><td>SERVER_IS_ANSIBLE_NODE=YES</td><td>
This parameter is only used if the SSHD_SUBSYSTEM_ALLOW parameter has been
used for the sftp subsystem. If used it identified the server as being
managed by ansible and as such must have sftp running, in this case the
warning for the sftp subsystem is altered to OK.
</td></tr>
<tr><td>ALLOW_AUTHORISED_KEYS=YES</td><td>
By default users with authorized_keys files are warnings, using this
parameter indicates it is OK for users to have those files and
suppresses the warnings.<br />
This does not affect the 'root' user as an alert will always be raised
if root has ssh keys; that should never be allowed.
</td></tr>
<tr><td>SUDOERS_ALLOW_ALL_SERVERS=value:<br />SUDOERS_ALLOW_ALL_COMMANDS=value:</td><td>
In all cases 'value' is the user or %group the sudoers rule is for.<br />
These paramaters can be used to downgrade alert severity levels with the following rules<br />
* if a rule is for all servers and all commands both parameters must be used to downgrade alert to warning<br />
* if a rule is for all servers but restricted commands allow_all_servers can downgrade from warning to OK<br />
* if a rule is for a specific server and all commands can downgrade from alert to warning with allow_all_commands<br />
* The trailing : is mandatory<br />
At no time will a rule allowing all commands be downgraded below a warning as it is a risk
</td></tr>
<tr><td>INCLUDE_CUSTOM_RULES=filename</td><td>
Include a custom rules file. These are intended to be rules defining the network ports,
file securites, new system owners etc. for an application. This allows rules for an
application to be defined in one file and 'included' in individual server custom
files rather than having to code the values for every server running the application.<br />
Where include files are used the values in the individual server custom file will
override values in the shared files in the cases where duplicate entries may result.
</td></tr>
<tr><td>BLUETOOTH_ALERT_TO_WARN=YES</td><td>
Downgrade bluetooth activity in the network checks from alerts to warnings
if bluetooth active sessions were found to be reported on
</td></tr>
<tr><td>ALLOW_OWNER_OVERRIDE=dirname:realowner:</td><td>
Only used in home directory checks where a shared directory is owned by someone
other than root. This was added for Debian11 where the sendmail director is a shared
home directory not owned by root. <b>The trailing : is required</b>.
Example of what is used in my 'include' common file
for Debian11 is<br />
ALLOW_OWNER_OVERRIDE=sendmail:smmta:
</td></tr>
<tr><td>DOCKER_ORPHANS_SUPPRESS=YES</td><td>
On servers running docker containers the UID and GID of directories and files
created are completely up to the creator of the container image.
Any well designed application container will avoid at all costs any UID/GID
that may already exist on the system. This does mean they show up as alerts
in the orphaned file report.
Using this parameter in a customisation file will suppress alerts for
any orphan files (files not owned by a existing system user) in 
docker container overlay directories
</td></tr>
<tr><td>SELINUX_NOT_INSTALLED=YES</td><td>
This will turn off the SELinux not installed alert as some servers may not
have SELinux. Even though I manually install it on my Debian servers a lot
of users probably do not go to that effort so this permits that alert to
be ignored/suppressed.
</td></tr>
<tr><td>NO_FIREWALL_INSTALLED=YES</td><td>
If a server has no iptables or netfilter rules this will change the no firewall check
from an alert to a warning. On some servers a firewall may not have been installed
(you always should) and this is needed for the SunOS checks at the moment as I
have not implemented tests for IPF (ipfilter) yet so those family servers will always
decide there is no firewall.
</td></tr>
<tr><td>UNSET_VAR=@var until first :@</td><td>
Added in version 0.25.<br />
For those rare cases where a default package may have been removed from a server
causing default custom rules for the server to alert on missing users, dirs, suid files
etc when using a generic server_OS base ruleset.<br />
This will comment out any custom entry where the text between the start and end @ bytes
exactly matches up to the first : field; the match text must end on a : or this
value will be ignored.
</td></tr>
</table>

<hr>
<a name="sect7"></a>
<center><a href="#sect6">[Previous Section]</a> <a href="#sect0">[Index]</a></center>
<H1>7. Appendix C - Quick Start Guide</H1>
<p>
This is the quick start guide for those who don't read
manuals. It explains how to setup and run it immediately.
</p>
<p>
You will have to at some point refer to the documentation
if you ever want to get down to a fairly error free
run, or to get it to run in 5 minutes instead of 50 minutes.
</p>
<table border="1" cellpadding="10">
<tr><td>
<b><big>1. Choose the central server</big></b><br>
Choose a server to be the one to provide the html results, it
should be running a web server such as apache. 
NOTE: A web server is not actually required as the output is
in flat file format (no cgi-bin stuff), but the output is as html reports so
the output must be places where a web browser can access it.<br />
<b>The server chosen must be secure</b>, the data collection scripts collect sensitive
information such as the contents of passwd and shadow files which you must not consolidate
on an externally facing server. Personally I do the processing on a grunty internal server
then just copy the results directory to a web server.
</td></tr>
<tr><td>
<b><big>2. Install all the scripts</big></b><br>
Untar the package on a central server
</td></tr>
<tr><td>
<b><big>3. Copy data collector script to all linux servers</big></b><br>
copy bin/collect_server_details.sh to every server<br />
(if you have many servers you probably use something like puppet/chef so you
should use as that will make it much easier to push out new versions to all servers)
</td></tr>
<tr><td>
<b><big>4. Run a data scan on each server to be reported on</big></b><br>
<ul>
<li>4.1 Run Initial Scan (default/max scanlevel)</li>
<ul>
<li>run "collect_server_details.sh" on each server copied to</li>
<li>scp/ftp the output .txt and .tar files created to a directory on the central server</li>
</ul>
<li>4.2 Frequent/Regular scans (override scanlevel)
To be done only after you have fixed up all system file perm errors.</li>
<ul>
<li>run "collect_server_details.sh --scanlevel=3" on each server copied to</li>
<li>scp/ftp the output .txt and .tar files created to a directory on the central server</li>
</ul>
</ul>

</td></tr>
<tr><td>
<b><big>5. Processing the output on the central server</big></b><br>
Where <em>dirname</em> is the directory you scp/ftp the files to on the central server
run "bin/process_server_details.sh <em>dirname</em>"
</td></tr>
<tr><td>
<b><big>6. Review results</big></b><br>
Results are accessable from results/index.html under the application directory.
</td></tr></table>


<hr>
<a name="sect8"></a>
<center><a href="#sect8">[Previous Section]</a> <a href="#sect0">[Index]</a></center>
<h1>8. Appendix D - Known Complications with using this toolkit</h1>
<p>
A customisation file per server is really required if you have multiple OS's.
For example some releases of Fedora moved bin and sbin under /, leaving symbolic links for /usr/bin and /usr/sbin
to point to the new directories under / <b>but</b> still creates system users in /etc/passwd with
home directories as /usr/bin and /usr/sbin. The impact of this is that home directory checks for
those users have to be overridden to permit lrwxrwxrwx as a 'secure'/safe setting, which it
obviously is not. <em>I may implement checks to cope with the use of symbolic links in this case at some point</em>,
but the issue is that if user home directories are set to symbolic links instead of directories the toolkit does not
handle that currently.
</p>
<p>
A customisation file per server is really required if you have multiple OS's.
CentOS7 still uses /usr/bin and /usr/sbin for files, so SUID file checks must be different
for each OS, Fedora will have a suid file at /bin/wall and CentOS7 will have it at /usr/bin/wall
so the ALL.custom file cannot be used for both OSs as files are in different places. This 
divergence gets even worse with BSD based systems. And CentOS7 and CentOS8 have different
suid files; just impossible to have a generic custom file for all servers.
</p>
<p>
There are also lots of critical alerts plus lots of warning generated on systems with <em>Docker</em>
running containers as in the docker overlay2 directory it keeps copies of setuid files for each container; plus with my
docker containers I keep the userid numbers well away from 'real' users on the host so there are
'bad ownership' alerts logged for user numbers as those numbers do not exist as valid users on the docker host.<br />
A customisation file entry exists to suppress the suid file alerts, it will however still generate a list of all
suid files that were suppressed from alerting so you can manually review them.<br />
A customisation file parameter also exists to suppress 'bad ownership' alerts for files under
'docker' directories if you get sick of seeing those.
</p>
<p>
There are also lots of critical alerts for Ubuntu distributions as these now supply many applications
as SNAP packages which have their own copies of SUID files.
A customisation file entry exists to suppress these suid file alerts, it will however still generate a list of all
suid files that were suppressed from alerting so you can manually review them.<br />
</p>
<p>
<b><em>None of these are issues with the toolkit itself but the effect of different *nix implementations</em></b>,
I chose not to code for all possible OS's but only the ones I use.
</p>
<p>
<b>Below are the know real issues with this toolkit</b>
</p>
<ul>
<li>Refer to the known issues in the KNOWN_ISSUES.txt file</li>
</ul>
<hr>
<center><a href="#sect0">[Index]</a></center>
</BODY>
</HTML>
